---
title: "Project 3: Effect of swarms' boundary value on computation"
output: html_notebook
---
Here we observe the effects of swarms' boundary value on computation.


```{sh}
#!/bin/bash

# v0.1.0
# 29.10.2019
# This script uses a fasta-file to measure computation time and memory of swarm 
# used with different boundary values and creates a .stats file for each boundary value.



FASTA_FILE="${1}"
SUBFOLDER="Test_stats"
FILE_NAME="${FASTA_FILE/.*/}"
mkdir -p "${SUBFOLDER}"
STATS="${SUBFOLDER}/${FILE_NAME}_b"


awk -F "_" '/^>/ {a=$2 - 1 ; if (a > 0) {print a}}' ${FASTA_FILE} | \
tac | \
while read b ; do 
	for i in {1..3} ; do # repeat it 3 times
		echo -ne "${FILE_NAME}\t${i}\t${b}\t" # shows boundary values as column next to other values
		/usr/bin/time -f "%e\t%M\t%P" \
		swarm -f -b "${b}" \
			"${FASTA_FILE}" \
			-s ${STATS}_${b}.stats \
			-o /dev/null \
			-l /dev/null 2>&1
	done 
paste - - < tmp.log >  TARA_V9_264_samples.log

exit 0
```

##We create a .log-table looking like this for each sample:

Sample                                          Iteration boundary-value Time(s) Memory(GB) CPU(%) Number_of_Clusters
neotropical_soils_18S_V4_175_samples_21_percent	1	        1	             23.64	 1046560	  643%	 1109687
neotropical_soils_18S_V4_175_samples_21_percent	2	        1	             22.05	 1046520	  657%	 1109687
neotropical_soils_18S_V4_175_samples_21_percent	3	        1	             22.43	 1046516	  655%	 1109687
neotropical_soils_18S_V4_175_samples_21_percent	1	        2	             149.19	 6435656	  1318%	 541076
neotropical_soils_18S_V4_175_samples_21_percent	2	        2	             134.20	 6435820	  1353%	 541076
neotropical_soils_18S_V4_175_samples_21_percent	3	        2	             135.74  6435744	  1374%	 541076

##Then we use these files to create a plot that shows the different computation values changing while increasing the boundary value:
```{r}
library(tidyverse)
library(scales)

setwd("C:\\Users\\Milena K\\OneDrive\\Documents\\Master_Biostudium\\Projekt_France")

input_18SV9 <- "TARA_V9_264_samples.log"
input_18SV4 <- "neotropical_soils_18S_V4_175_samples_21_percent.log"
output <- "18S_V4_and_V9_fastidious_computation_parameters.pdf"
col_names <- c("Sample", "Iteration", "boundary_value", "Time (s)",
               "Memory (GB)", "CPU", "Number of Clusters (in thousands)")

## synopsys:
## - merge V4 and V9 datasets,
## - average iteration (group by boundary value),
## - remove legend,
## - convert memory into gigabytes,
## - convert number of clusters into kilos,
## - compute mean per iteration,
## - remove y-axis label,
## - use comma for the y-axis,
## - give different alpha-values

bind_rows(read_tsv(input_18SV4, col_names = col_names) %>%
            mutate(marker = "18S V4"),
          read_tsv(input_18SV9, col_names = col_names) %>%
            mutate(marker = "18S V9")) %>%
  mutate(`CPU (%)` = parse_number(CPU)) %>%
  select (-CPU) %>%
  gather("variables", "values", c(`Time (s)`, `Memory (GB)`,
                                  `CPU (%)`, `Number of Clusters (in thousands)`)) %>%
  mutate(values = case_when (variables == "Memory (GB)" ~ values / 2^20,
                             variables == "Number of Clusters (in thousands)" ~ values / 1000,
                             TRUE ~ values)) %>%
  group_by(marker, boundary_value, variables) %>%
  mutate(values = mean(values)) %>%
  ungroup() %>%
  select(-Iteration) %>%
  unique() ->  d

ggplot(data = d, aes(x = boundary_value, y = values, colour = marker, alpha = marker)) +
  geom_point(size = 1.0) +
  expand_limits(y = 0) +
  theme_bw(base_size = 16) +
  scale_alpha_manual(values = c(0.8, 0.08)) +
  scale_x_log10(name = "boundary value (log)",
                breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  scale_y_continuous(labels = comma) +
  labs(x = "boundary value") +
  theme(axis.title.y = element_blank(),
        legend.title = element_blank(),
        legend.justification = c(1, 0),
        legend.position = c(0.95, 0.05)) +
  facet_wrap(~ variables, scales = "free")

#ggsave(output, width = 16, height = 9)
```

On the x-axis we see the boundary value on a log scale. On the y-axis, CPU in percent, the Memory in GB, the number of clusters in thousands and the time in seconds of 18S V4 data (in pink) and 18S V9 data (in blue) is plotted. 18S V4 data is about 30 kb, 18S V9 data 1,419 kb.
The V4 sequences are 380 nucleotide long on average, and V9 sequences are 130 nucleotide long on average. The V4 dataset was subsampled to the same size (99%) in bytes than the V9 dataset to ensure equal conditions for measuring, because computation would probably be faster on shorter nucleotides.

In general you can see that the lowest time, memory and CPU values and the highest number of cluster value are bound with the lowest boundary value (1 = no fastidious option) in both datasets, whereas only the max memory value is bound to the max boundary value (724731 V4; 15638315 V9) in both datasets. The highest peaks in time and CPU and the lowest number of clusters are not bound with the max boundary value, but rather rise to a certain level and then fall again. 
Also interesting could be the values where the boundary value (bv) is equal 2: in comparison to bv == 1, bv == 2 needs at least three times longer, the CPU is doubled, there is at least three times more memory needed and the number of clusters are cut down to a half.
The max time is 154.27 on 3866 boundary value (V4) and 158.39 on 89624 boundary value (V9). The max CPU is 1381% on 1252 boundary value (V4) and 1281% on 13031 boundary value (V9). The min Number of Clusters is 533961 on 3 boundary value (V4) and 906547 on 11 boundary value (V9). 
V4 and V9 compared, computation on V4 data results in a lower number of clusters, but needs more CPU, more memory and longer time.


```{r}
log_18SV9 <- read_tsv("TARA_V9_264_samples.log", col_names = col_names)
log_18SV4 <- read_tsv("neotropical_soils_18S_V4_175_samples_21_percent.log", col_names = col_names)
```

##Find the means of the three Iterations:
```{r}
colnames <- c("boundary_value", "time", "memory", "CPU", "number_clusters")
log_18SV9_mean <- as.data.frame(cbind(colMeans(matrix(log_18SV9$boundary_value, nrow=3)),
                                    colMeans(matrix(log_18SV9$`Time (s)`, nrow=3)),
                                    colMeans(matrix(log_18SV9$`Memory (GB)`, nrow=3)),
                                    colMeans(matrix(parse_number(log_18SV9$CPU), nrow=3)),
                                    colMeans(matrix(log_18SV9$`Number of Clusters (in thousands)`, nrow=3))))
colnames(log_18SV9_mean) <- colnames     

log_18SV4_mean <- as.data.frame(cbind(colMeans(matrix(log_18SV4$boundary_value, nrow=3)),
                                    colMeans(matrix(log_18SV4$`Time (s)`, nrow=3)),
                                    colMeans(matrix(log_18SV4$`Memory (GB)`, nrow=3)),
                                    colMeans(matrix(parse_number(log_18SV4$CPU), nrow=3)),
                                    colMeans(matrix(log_18SV4$`Number of Clusters (in thousands)`, nrow=3))))
colnames(log_18SV4_mean) <- colnames
```

Find max and mins from the mean of the iterations:
```{r}
values_mean <- as.data.frame(rbind(log_18SV4_mean[which.max(log_18SV4_mean[,2]), ],
                                   log_18SV4_mean[which.max(log_18SV4_mean[,3]), ],
                                   log_18SV4_mean[which.max(log_18SV4_mean[,4]), ],
                                   log_18SV4_mean[which.max(log_18SV4_mean[,5]), ],
                                   log_18SV4_mean[which.max(log_18SV4_mean[,1]), ],
                                   log_18SV4_mean[which.min(log_18SV4_mean[,2]), ],
                                   log_18SV4_mean[which.min(log_18SV4_mean[,3]), ],
                                   log_18SV4_mean[which.min(log_18SV4_mean[,4]), ],
                                   log_18SV4_mean[which.min(log_18SV4_mean[,5]), ],
                                   log_18SV4_mean[which.min(log_18SV4_mean[,1]), ],
                                   log_18SV9_mean[which.max(log_18SV9_mean[,2]), ],
                                   log_18SV9_mean[which.max(log_18SV9_mean[,3]), ],
                                   log_18SV9_mean[which.max(log_18SV9_mean[,4]), ],
                                   log_18SV9_mean[which.max(log_18SV9_mean[,5]), ],
                                   log_18SV9_mean[which.max(log_18SV9_mean[,1]), ],
                                   log_18SV9_mean[which.min(log_18SV9_mean[,2]), ],
                                   log_18SV9_mean[which.min(log_18SV9_mean[,3]), ],
                                   log_18SV9_mean[which.min(log_18SV9_mean[,4]), ],
                                   log_18SV9_mean[which.min(log_18SV9_mean[,5]), ],
                                   log_18SV9_mean[which.min(log_18SV9_mean[,1]), ]))

values_mean$column <- c("Time_max", "Memory_max", "CPU_max", "Number of Clusters_max", 
                        "Boundary Value_max", "Time_min", "Memory_min", "CPU_min", 
                        "Number of Clusters_min", "Boundary Value_min")

values_mean$samples <- c(rep("18S_V4", 10), rep("18S_V9", 10))
```

Find the values of the boundary value 2 concerning the mean of the iterations
```{r}

boundary_value_2_mean <- rbind(log_18SV4_mean[which.min(log_18SV4_mean$boundary_value), ],
                               log_18SV9_mean[which.min(log_18SV9_mean$boundary_value), ], 
                               log_18SV4_mean[which(log_18SV4_mean$boundary_value == 2), ],
                               log_18SV9_mean[which(log_18SV9_mean$boundary_value == 2), ])

boundary_value_2_mean$Sample <- c("18S_V4", "18S_V9", "18S_V4", "18S_V9")
```



(Find max and mins of all iterations
```{r}
values <- rbind(log_18SV4[which.max(log_18SV4$`Time (s)`), ],
                log_18SV4[which.max(log_18SV4$`Memory (GB)`), ],
                log_18SV4[which.max(parse_number(log_18SV4$CPU)), ],
                log_18SV4[which.max(log_18SV4$`Number of Clusters (in thousands)`), ],
                log_18SV4[which.max(log_18SV4$boundary_value), ],
                log_18SV4[which.min(log_18SV4$`Time (s)`), ],
                log_18SV4[which.min(log_18SV4$`Memory (GB)`), ],
                log_18SV4[which.min(parse_number(log_18SV4$CPU)), ],
                log_18SV4[which.min(log_18SV4$`Number of Clusters (in thousands)`), ],
                log_18SV4[which.min(log_18SV4$boundary_value), ],
                log_18SV9[which.max(log_18SV9$`Time (s)`), ],
                log_18SV9[which.max(log_18SV9$`Memory (GB)`), ],
                log_18SV9[which.max(parse_number(log_18SV9$CPU)), ],
                log_18SV9[which.max(log_18SV9$`Number of Clusters (in thousands)`), ],
                log_18SV9[which.max(log_18SV9$boundary_value), ],
                log_18SV9[which.min(log_18SV9$`Time (s)`), ],
                log_18SV9[which.min(log_18SV9$`Memory (GB)`), ],
                log_18SV9[which.min(parse_number(log_18SV9$CPU)), ],
                log_18SV9[which.min(log_18SV9$`Number of Clusters (in thousands)`), ],
                log_18SV9[which.min(log_18SV9$boundary_value), ])

values$Column <- c("Time_max", "Memory_max", "CPU_max", "Number of Clusters_max", 
                   "Boundary Value_max", "Time_min", "Memory_min", "CPU_min", 
                   "Number of Clusters_min", "Boundary Value_min")
values <- values[,c(1,8,2,4,3,5,6,7)]
```
)


##Here we compare computation of V4 and V9 data where the boundary values are equal, to have an equal sample size:
```{r}
equal_boundary_values_V9 <- rbind(log_18SV9_mean[which(log_18SV4_mean$boundary_value == log_18SV9_mean$boundary_value), ])
equal_boundary_values_V4 <- rbind(log_18SV4_mean[which(log_18SV4_mean$boundary_value == log_18SV9_mean$boundary_value), ])
```

```{r}
shapiro.test(equal_boundary_values_V9$time)
shapiro.test(equal_boundary_values_V9$memory)
shapiro.test(equal_boundary_values_V9$CPU)
shapiro.test(equal_boundary_values_V9$number_clusters)
```
The Shapiro-Wilk normality test tells us that the variables are not normally distributed.
This is why we have to use the Mann-Whitney-Wilcoxon Test for independent data that doesn't follow the normal distribution.
```{r}
wilcox.test(equal_boundary_values_V9$time, equal_boundary_values_V4$time)
wilcox.test(equal_boundary_values_V9$memory, equal_boundary_values_V4$memory)
wilcox.test(equal_boundary_values_V9$CPU, equal_boundary_values_V4$CPU)
wilcox.test(equal_boundary_values_V9$number_clusters, equal_boundary_values_V4$number_clusters)
```
Because the shapiro test tells us that our variables are not normally distributed, we compare the values by the Mann-Whitney-Wilcoxon Test. We compare the same boundary values and the tests tell us that the difference between the datasets concerning time, memory, CPU and Number of Clusters at these boundary values are significant (significance level = 0.05).

## Comparison visualized in box plots
```{r}
samples <- c(rep("18S_V4", 16), rep("18S_V9", 16))
joined_equal_values <- full_join(equal_boundary_values_V4, equal_boundary_values_V9) %>% 
                       add_column(samples) %>% 
                       gather("variables", "values", c(time, memory, CPU, number_clusters))

ggplot(joined_equal_values,
       aes(x = samples,
           y = values,
           fill = samples)) +
  geom_boxplot() +
  theme_bw(base_size = 16) +
  theme(legend.position = "none") +
  labs(title= "Computation of 18S Data in swarm compared", x = NULL, y = NULL) +
  scale_x_discrete(labels = c("V4", "V9")) +
  scale_fill_manual(values = c("coral", "coral3")) +
  facet_wrap(~ variables, scales = "free")

#ggsave("Comparision_equal_boundary_values.pdf", width = 16, height = 9)
```
